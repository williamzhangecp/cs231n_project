{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "csv_file = open('cs231n/datasets/fer2013/fer2013.csv')\n",
    "\n",
    "reader_file = csv.reader(csv_file)\n",
    "\n",
    "def read_faces_csv(reader_file, center=True):\n",
    "\t\"\"\"\n",
    "\tFunction that takes as input file a csv.reader() instance and assumes the following formatting:\n",
    "\temotion, pixels (2034 of them), usage (train, test, val)\n",
    "\tReturns the following numpy arrays:\n",
    "\t- X_train, y_train (respectively (N, 48, 48), (N,) representing raw grayscale pixels and emotion labels)\n",
    "\t- X_test, y_test\n",
    "\t- X_val, y_val\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Discard header\n",
    "\trow = next(reader_file)\n",
    "\n",
    "\tX_train_list, y_train_list = [], []\n",
    "\tX_test_list, y_test_list = [], []\n",
    "\tX_val_list, y_val_list = [], []\n",
    "\n",
    "\tN_train, N_test, N_val = 0, 0, 0\n",
    "\n",
    "\tfor row in reader_file:\n",
    "\t\ty_str, X_row_str, data_type = row\n",
    "\t\ty = int(y_str)\n",
    "\n",
    "\t\tX_row_strs = X_row_str.split(' ')\n",
    "\t\tX_row = [float(x) for x in X_row_strs]\n",
    "\t\t\n",
    "\t\tif data_type == 'PublicTest':\n",
    "\t\t\ty_test_list.append(y)\n",
    "\t\t\tX_test_list.append(X_row)\n",
    "\t\t\tN_test += 1\n",
    "\t\telif data_type == 'PrivateTest':\n",
    "\t\t\ty_val_list.append(y)\n",
    "\t\t\tX_val_list.append(X_row)\n",
    "\t\t\tN_val += 1\n",
    "\t\telse:\n",
    "\t\t\ty_train_list.append(y)\n",
    "\t\t\tX_train_list.append(X_row)\n",
    "\t\t\tN_train += 1\n",
    "\n",
    "\tX_train = np.asarray(X_train_list).astype('float64').reshape((N_train, 48, 48))\n",
    "\ty_train = np.asarray(y_train_list)\n",
    "\n",
    "\tX_test = np.asarray(X_test_list).astype('float64').reshape((N_test, 48, 48))\n",
    "\ty_test = np.asarray(y_test_list)\n",
    "\n",
    "\tX_val = np.asarray(X_val_list).astype('float64').reshape((N_val, 48, 48))\n",
    "\ty_val = np.asarray(y_val_list)\n",
    "\n",
    "\t# decide to mean-center or not\n",
    "\tif center:\n",
    "\t\ttrain_mean = X_train.mean(axis = 0)\n",
    "\t\tX_train -= train_mean\n",
    "\t\tX_test -= train_mean\n",
    "\t\tX_val -= train_mean\n",
    "\n",
    "\t#########\n",
    "\treturn X_train, y_train, X_test, y_test, X_val, y_val\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val = read_faces_csv(reader_file)\n",
    "\n",
    "#(28709, 48, 48) (28709,) (3589, 48, 48) (3589,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 48, 48) (28709,) (3589, 48, 48) (3589,)\n",
      "(Iteration 1 / 30) loss: 2.294224\n",
      "(Epoch 0 / 15) train acc: 0.160000; val_acc: 0.165506\n",
      "(Iteration 2 / 30) loss: 2.409355\n",
      "(Epoch 1 / 15) train acc: 0.170000; val_acc: 0.147116\n",
      "(Iteration 3 / 30) loss: 1.947595\n",
      "(Iteration 4 / 30) loss: 1.726817\n",
      "(Epoch 2 / 15) train acc: 0.320000; val_acc: 0.168013\n",
      "(Iteration 5 / 30) loss: 1.753057\n",
      "(Iteration 6 / 30) loss: 1.849576\n",
      "(Epoch 3 / 15) train acc: 0.390000; val_acc: 0.193926\n",
      "(Iteration 7 / 30) loss: 1.514765\n",
      "(Iteration 8 / 30) loss: 1.539288\n",
      "(Epoch 4 / 15) train acc: 0.460000; val_acc: 0.179159\n",
      "(Iteration 9 / 30) loss: 1.428239\n",
      "(Iteration 10 / 30) loss: 1.299156\n",
      "(Epoch 5 / 15) train acc: 0.470000; val_acc: 0.218445\n",
      "(Iteration 11 / 30) loss: 1.526907\n",
      "(Iteration 12 / 30) loss: 1.278815\n",
      "(Epoch 6 / 15) train acc: 0.650000; val_acc: 0.209808\n",
      "(Iteration 13 / 30) loss: 1.157935\n",
      "(Iteration 14 / 30) loss: 1.128510\n",
      "(Epoch 7 / 15) train acc: 0.710000; val_acc: 0.236835\n",
      "(Iteration 15 / 30) loss: 0.886735\n",
      "(Iteration 16 / 30) loss: 0.943601\n",
      "(Epoch 8 / 15) train acc: 0.750000; val_acc: 0.209529\n",
      "(Iteration 17 / 30) loss: 0.820637\n",
      "(Iteration 18 / 30) loss: 0.822632\n",
      "(Epoch 9 / 15) train acc: 0.780000; val_acc: 0.194205\n",
      "(Iteration 19 / 30) loss: 0.701598\n",
      "(Iteration 20 / 30) loss: 0.604578\n",
      "(Epoch 10 / 15) train acc: 0.860000; val_acc: 0.215938\n",
      "(Iteration 21 / 30) loss: 0.295466\n",
      "(Iteration 22 / 30) loss: 0.419594\n",
      "(Epoch 11 / 15) train acc: 0.920000; val_acc: 0.207021\n",
      "(Iteration 23 / 30) loss: 0.233339\n",
      "(Iteration 24 / 30) loss: 0.299576\n",
      "(Epoch 12 / 15) train acc: 0.950000; val_acc: 0.222903\n",
      "(Iteration 25 / 30) loss: 0.187016\n",
      "(Iteration 26 / 30) loss: 0.177279\n",
      "(Epoch 13 / 15) train acc: 0.980000; val_acc: 0.207857\n",
      "(Iteration 27 / 30) loss: 0.253298\n",
      "(Iteration 28 / 30) loss: 0.080520\n",
      "(Epoch 14 / 15) train acc: 0.970000; val_acc: 0.197548\n",
      "(Iteration 29 / 30) loss: 0.125507\n",
      "(Iteration 30 / 30) loss: 0.153396\n",
      "(Epoch 15 / 15) train acc: 0.990000; val_acc: 0.226804\n"
     ]
    }
   ],
   "source": [
    "N, D, D = X_train.shape\n",
    "N_val = X_val.shape[0]\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "X_train2 = np.zeros((N,3,D,D))\n",
    "X_val2 = np.zeros((N_val,3,D,D))\n",
    "X_test2 = np.zeros((N_test,3,D,D))\n",
    "\n",
    "#convert graysclae to rgb\n",
    "for i in xrange(N):\n",
    "    for j in xrange(3):\n",
    "        X_train2[i,j,:,:] = X_train[i,:,:]\n",
    "for i in xrange(N_val):\n",
    "    for j in xrange(3):\n",
    "        X_val2[i,j,:,:] = X_val[i,:,:]\n",
    "for i in xrange(N_test):\n",
    "    for j in xrange(3):\n",
    "        X_test2[i,j,:,:] = X_test[i,:,:]\n",
    "        \n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': X_train2[:num_train],\n",
    "  'y_train': y_train[:num_train],\n",
    "  'X_val': X_val2,\n",
    "  'y_val': y_val,\n",
    "}\n",
    "\n",
    "print X_train.shape, y_train.shape, X_val.shape, y_test.shape\n",
    "model = ThreeLayerConvNet(weight_scale=5e-3)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=15, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 5e-4,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 20) loss: 2.294817\n",
      "(Epoch 0 / 10) train acc: 0.170000; val_acc: 0.147116\n",
      "(Iteration 2 / 20) loss: 2.236982\n",
      "(Epoch 1 / 10) train acc: 0.270000; val_acc: 0.149067\n",
      "(Iteration 3 / 20) loss: 1.855226\n",
      "(Iteration 4 / 20) loss: 1.931032\n",
      "(Epoch 2 / 10) train acc: 0.310000; val_acc: 0.225968\n",
      "(Iteration 5 / 20) loss: 1.770074\n",
      "(Iteration 6 / 20) loss: 1.674149\n",
      "(Epoch 3 / 10) train acc: 0.420000; val_acc: 0.214544\n",
      "(Iteration 7 / 20) loss: 1.692839\n",
      "(Iteration 8 / 20) loss: 1.502859\n",
      "(Epoch 4 / 10) train acc: 0.510000; val_acc: 0.197827\n",
      "(Iteration 9 / 20) loss: 1.522390\n",
      "(Iteration 10 / 20) loss: 1.405718\n",
      "(Epoch 5 / 10) train acc: 0.430000; val_acc: 0.181109\n",
      "(Iteration 11 / 20) loss: 1.192431\n",
      "(Iteration 12 / 20) loss: 1.401796\n",
      "(Epoch 6 / 10) train acc: 0.530000; val_acc: 0.186403\n",
      "(Iteration 13 / 20) loss: 1.157968\n",
      "(Iteration 14 / 20) loss: 0.933619\n",
      "(Epoch 7 / 10) train acc: 0.640000; val_acc: 0.209250\n",
      "(Iteration 15 / 20) loss: 1.039076\n",
      "(Iteration 16 / 20) loss: 0.871343\n",
      "(Epoch 8 / 10) train acc: 0.660000; val_acc: 0.207579\n",
      "(Iteration 17 / 20) loss: 0.766481\n",
      "(Iteration 18 / 20) loss: 1.016072\n",
      "(Epoch 9 / 10) train acc: 0.750000; val_acc: 0.247144\n",
      "(Iteration 19 / 20) loss: 0.618025\n",
      "(Iteration 20 / 20) loss: 0.769494\n",
      "(Epoch 10 / 10) train acc: 0.780000; val_acc: 0.207857\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  'X_train': X_train2,\n",
    "  'y_train': y_train,\n",
    "  'X_val': X_val2,\n",
    "  'y_val': y_val,\n",
    "  'X_test': X_test2,\n",
    "  'y_test': y_test,\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=5e-3)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=10, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 5e-4,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(model.loss(data['X_val']), axis=1)\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
